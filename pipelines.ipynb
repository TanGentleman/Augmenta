{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 8 step process to display a goal value\n",
    "1. Get prompt as string\n",
    "2. Chain1: LCEL that guarantees a schema that fits step 3, or fails\n",
    "3. Perform Tavily query\n",
    "4. Chain2: LCEL that extracts most relevant url\n",
    "5. Load documents into a docs object\n",
    "6. Embed docs as a vectorstore and create a retriever\n",
    "7. Chain3: LCEL that guarantees a schema that fits step 8\n",
    "8. Display the goal value\n",
    "\n",
    "For this task, the prompt is a specific question about a given college. The goal value results from the highest quality source publically available, typically the common data set officially released by the university."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from langchain.tools import DuckDuckGoSearchRun\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from models import get_together_fn_mix, get_together_fn_mistral\n",
    "ACTIVE_LLM = get_together_fn_mix()\n",
    "# ACTIVE_LLM = get_together_fn_mistral()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "# Get prompt as string\n",
    "USER_QUERY = \"Is West Valley a good school?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1_examples = [\n",
    "    {\"input\": \"What are admissions rates for CMU?\", \"output\": \"[{'name': 'SearchCollege', 'arguments': {'valid': True, 'query': 'admissions rates for Carnegie Mellon University', 'college': 'Carnegie Mellon University'}}]\"},\n",
    "    {\"input\": \"How many rings does Saturn have?\", \"output\": \"[{'name': 'SearchCollege', 'arguments': {'valid': False, 'query': 'number of Saturn rings', 'college': 'None'}}]\"},\n",
    "    {\"input\": \"Who created the super soaker?\", \"output\": \"[{'name': 'SearchCollege', 'arguments': {'valid': False, 'query': 'Who created the super soaker', 'college': 'None'}}]\"},\n",
    "    {\"input\": \"What is De Anza College like?\", \"output\": \"[{'name': 'SearchCollege', 'arguments': {'valid': True, 'query': 'De Anza College', 'college': 'De Anza College'}}]\"},\n",
    "    {\"input\": \"What is the acceptance rate for Stanford?\", \"output\": \"[{'name': 'SearchCollege', 'arguments': {'valid': True, 'query': 'acceptance rate for Stanford University', 'college': 'Stanford University'}}]\"},\n",
    "]\n",
    "def get_few_shot_prompt(examples):\n",
    "    # This is a prompt template used to format each individual example.\n",
    "    example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"ai\", \"{output}\"),\n",
    "        ]\n",
    "    )\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "    )\n",
    "    return few_shot_prompt\n",
    "# print(get_few_shot_prompt(chain1_examples).format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1_few_shot_prompt = get_few_shot_prompt(chain1_examples)\n",
    "CHAIN1_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant who always calls an external function. If a valid college is not found in the query, mark the valid field as False.\"), \n",
    "     chain1_few_shot_prompt,\n",
    "     (\"human\", \"{input}\")]\n",
    ")\n",
    "PARSER = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2:\n",
    "# Chain1: LCEL that guarantees a schema that fits step 3, or fails\n",
    "class SearchCollege(BaseModel):\n",
    "    \"\"\"Construct a web search for a college. Only mark valid field as true if a valid college name is found.\"\"\"\n",
    "    valid: bool = Field(description=\"Mark as false if a college name is not found in the query.\")\n",
    "    query: str = Field(description=\"Optimized query for search engine (include college name if present)\")\n",
    "    college: str = Field(description=\"Name of the college.\")\n",
    "def is_chain1_valid(response_object):\n",
    "    return response_object[0]['arguments']['valid'] is True\n",
    "\n",
    "tools = [convert_to_openai_tool(SearchCollege)]\n",
    "tool_choice={\"type\": \"function\", \"function\": {\"name\": \"SearchCollege\"}}\n",
    "chain1 = CHAIN1_PROMPT | ACTIVE_LLM.bind(tools=tools, tool_choice=tool_choice)\n",
    "print(\"Successfully constructed chain1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call chain1\n",
    "chain1_prompt = USER_QUERY\n",
    "response1 = chain1.invoke({\"input\": chain1_prompt})\n",
    "try:\n",
    "    response_object1 = PARSER.parse(response1.content)\n",
    "except:\n",
    "    raise ValueError(\"Chain1 did not return valid JSON\")\n",
    "if not is_chain1_valid(response_object1):\n",
    "    raise ValueError(\"Chain1 failed schema conditions\")\n",
    "# print(response_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3:\n",
    "# Perform Tavily query\n",
    "TAVILY_MAX_RESULTS = 15\n",
    "search = TavilySearchResults(max_results=TAVILY_MAX_RESULTS)\n",
    "def get_tavily_query(response_object):\n",
    "    assert is_chain1_valid(response_object), \"Chain1 output is not valid\"\n",
    "    college = response_object[0]['arguments']['college']\n",
    "    query = college + \" Common Data Set 2023 filetype:pdf\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize Tavily query\n",
    "print(response_object1)\n",
    "final_query = get_tavily_query(response_object1)\n",
    "print(final_query)\n",
    "COLLEGE_NAME = response_object1[0]['arguments']['college']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Tavily search\n",
    "results = search.invoke(final_query)\n",
    "print(f\"Top {TAVILY_MAX_RESULTS} results from Tavily:\")\n",
    "for res in results:\n",
    "    print(res[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2_examples = [\n",
    "    {\n",
    "        \"input\": \"\"\"Choose the most relevant url index for the latest released data by Lehigh University: \n",
    "0: https://data.lehigh.edu/sites/oirsa.lehigh.edu/files/CDS_2021-2022.pdf\n",
    "1: https://www.williams.edu/institutional-research/files/2023/04/CDS_2022_2023_Williams_March2023.pdf\n",
    "2: https://my.wlu.edu/document/2022-common-data-set\n",
    "3: https://data.lehigh.edu/sites/oirsa.lehigh.edu/files/CDS_2022-2023.pdf\n",
    "4: https://www.haverford.edu/sites/default/files/Office/President/CDS_2022-2023.pdf\"\"\", \n",
    "        \"output\": \"[{'name': 'selectUrl', 'arguments': {'index': 3}}]\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2_few_shot_prompt = get_few_shot_prompt(chain2_examples)\n",
    "CHAIN2_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful assistant who always calls an external function. Choose the correct url for the given college.\"),\n",
    "     chain2_few_shot_prompt,\n",
    "     (\"human\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:\n",
    "# Extract relevant url(s)\n",
    "def format_url_choice_string(urls, college):\n",
    "    return f\"Choose the most relevant url index for the latest released data by {college}: \\n\" + \"\\n\".join([f\"{i}: {url}\" for i, url in enumerate(urls)])\n",
    "chain2_needed = False\n",
    "chain2 = None\n",
    "if not results:\n",
    "    raise ValueError(\"No results found\")\n",
    "urls = [result['url'] for result in results]\n",
    "assert len(urls) > 0, \"No urls found\"\n",
    "if len(urls) == 1:\n",
    "    print('Skipping chain2')\n",
    "else:\n",
    "    print('Selecting the best url using chain2')\n",
    "    chain2_needed = True\n",
    "\n",
    "if chain2_needed:\n",
    "    class selectUrl(BaseModel):\n",
    "        \"\"\"Choose the most relevant url index for the given college.\"\"\"\n",
    "        index: int = Field(description=\"The index of the url to select\")\n",
    "    select_url_tools = [convert_to_openai_tool(selectUrl)]\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"selectUrl\"}}\n",
    "    chain2 = CHAIN2_PROMPT | ACTIVE_LLM.bind(tools=tools, tool_choice=tool_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call chain2 if needed to select the most relevant url\n",
    "# This chain only performs well with the MIXTRAL model, and not the MISTRAL model\n",
    "relevant_url = None\n",
    "if chain2 is not None:\n",
    "    chain2_prompt = format_url_choice_string(urls, COLLEGE_NAME)\n",
    "    print(chain2_prompt)\n",
    "    response2 = chain2.invoke({\"input\": chain2_prompt})\n",
    "    try:\n",
    "        response_object2 = PARSER.parse(response2.content)\n",
    "    except:\n",
    "        raise ValueError(\"Chain2 did not return valid JSON\")\n",
    "    chain_2_response_index = response_object2[0]['arguments']['index']\n",
    "    # print(response_object)\n",
    "    assert chain_2_response_index < len(urls), \"Index out of range\"\n",
    "    relevant_url = urls[chain_2_response_index]\n",
    "else:\n",
    "    relevant_url = urls[0]\n",
    "print('\\n' + relevant_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5:\n",
    "# Load documents into a docs object\n",
    "loader = PyPDFLoader(relevant_url)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pages), \"pages loaded\")\n",
    "for page in pages:\n",
    "    print(page.page_content[:500])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
