{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_together.embeddings import TogetherEmbeddings\n",
    "\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "from helpers import get_vectorstore, save_response_to_markdown_file, read_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_together_nous_mix, get_together_quen\n",
    "ACTIVE_LLM = get_together_nous_mix()\n",
    "def get_retriever(filename, context_size = \"8k\"):\n",
    "    model = f\"togethercomputer/m2-bert-80M-{context_size}-retrieval\"\n",
    "    embedder = TogetherEmbeddings(model=model)\n",
    "    local_vector_path = f\"{filename[:-4]}-embeddings\"\n",
    "\n",
    "    vectorstore = get_vectorstore(embedder = embedder, local_vector_path = local_vector_path)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILENAME = \"yang.pdf\"\n",
    "from os.path import exists\n",
    "assert exists(f\"vector-dbs/{PDF_FILENAME[:-4]}-embeddings\"), \"Embeddings not found. Run DocQuery first.\"\n",
    "retriever = get_retriever(PDF_FILENAME, context_size=\"8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKGROUND_OG = \"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\"\n",
    "standalone_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(standalone_template)\n",
    "\n",
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(rag_template)\n",
    "ANSWER_PROMPT.messages.insert(0, \n",
    "   SystemMessage(\n",
    "       content=\"You are a precise, autoregressive question-answering system.\"\n",
    "   )\n",
    "  )\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"# _inputs = RunnableParallel(\n",
    "#     standalone_question=RunnablePassthrough.assign(\n",
    "#         chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "#     )\n",
    "#     | CONDENSE_QUESTION_PROMPT\n",
    "#     | ACTIVE_LLM\n",
    "#     | StrOutputParser(),\n",
    "# )\n",
    "# _context = {\n",
    "#     \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "#     \"question\": lambda x: x[\"standalone_question\"],\n",
    "# }\n",
    "# conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ACTIVE_LLM\n",
    "# def get_conversational_qa_response(question: str, chat_history: list = None):\n",
    "#     conversational_chat_history = chat_history\n",
    "#     if conversational_chat_history is None:\n",
    "#         conversational_chat_history = ChatMessageHistory()\n",
    "#     response = conversational_qa_chain.invoke(\n",
    "#         {\n",
    "#             \"question\": question,\n",
    "#             \"chat_history\": conversational_chat_history,\n",
    "#         }\n",
    "#     )\n",
    "#     return response\n",
    "\n",
    "# history = ChatMessageHistory()\n",
    "# history.add_user_message(\"Only answer Yes or No. Does this paper discuss chromosomes?\")\n",
    "# history.add_ai_message(\"No\")\n",
    "\n",
    "# query = \"Tell me more the main ideas it does discuss.\"\n",
    "# response = get_conversational_qa_response(question = query, chat_history = history.messages)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, input_key=\"question\", output_key=\"answer\"\n",
    ")\n",
    "chat_context = {\"question\": \"Only answer Yes or No. Does this paper discuss chromosomes?\"}, {\"answer\": \"Yes\"}\n",
    "memory.save_context(*chat_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer_as_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, input_key=\"question\", output_key=\"answer\"\n",
    ")\n",
    "loaded_memory_reg = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "regular_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | loaded_memory_reg\n",
    "    | ANSWER_PROMPT\n",
    "    | ACTIVE_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "sample_prompt = read_sample()\n",
    "response = regular_chain.invoke(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": sample_prompt}\n",
    "memory.save_context(inputs, {\"answer\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ACTIVE_LLM\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ACTIVE_LLM,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP \n",
    "# QUESTION = f\"\"\"Given the following instructions, help me create specific steps to test and adjust the retrieval threshold.\n",
    "# {read_sample()}\n",
    "# \"\"\"\n",
    "# inputs = {\"question\": QUESTION}\n",
    "# result = final_chain.invoke(inputs)\n",
    "# memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION = f\"\"\"Given the following instructions, help me create specific steps to test and adjust the retrieval threshold.\n",
    "# {read_sample()}\n",
    "# \"\"\"\n",
    "QUESTION = read_sample()\n",
    "inputs = {\"question\": QUESTION}\n",
    "result = final_chain.invoke(inputs)\n",
    "memory.save_context(inputs, {\"answer\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_response_to_markdown_file(response.content, \"response.md\")\n",
    "save_response_to_markdown_file(response, \"response.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history_from_memory(chat_memory):\n",
    "    msgs = chat_memory.buffer_as_str.split(\"\\n\")\n",
    "    count = 1\n",
    "    text = \"\"\n",
    "    for msg in msgs:\n",
    "        text += msg + \"\\n\"\n",
    "        count += 1\n",
    "    print(text)\n",
    "\n",
    "print_history_from_memory(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW_QUESTION = f\"\"\"Given the following instructions, help me create specific steps to test and adjust the retrieval threshold.\n",
    "# {read_sample()}\n",
    "# \"\"\"\n",
    "NEW_QUESTION = read_sample()\n",
    "inputs = {\"question\": NEW_QUESTION}\n",
    "result = final_chain.invoke(inputs)\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
