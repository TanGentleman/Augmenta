{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plan:\n",
    "Input files: transcript.txt, transcript.json\n",
    "1. Choose chunk size / overlap for the transcript and create a vectorstore\n",
    "3. Create a function that accepts a query and returns the most relevant section(s) of text\n",
    "4. Use transcript.json to also return the associated timestamps for the relevant sections\n",
    "5. Modify the interface to be like chat.py, where the transcript can be \"read\" and queries typed/pasted, with outputs being the appropriate doc excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_together.embeddings import TogetherEmbeddings\n",
    "\n",
    "\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import uuid\n",
    "from os.path import exists\n",
    "# import json\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import SystemMessage#, AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import EMBEDDING_CONTEXT_SIZE, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "from helpers import get_vectorstore, save_vector, save_response_to_markdown_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_together_fn_mix, get_claude_opus\n",
    "ACTIVE_LLM = get_claude_opus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"example-simple.txt\"),\n",
    "    # TextLoader(\"example.txt\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"paper-embeddings\"\n",
    "embedder = TogetherEmbeddings(model=\"BAAI/bge-large-en-v1.5\")\n",
    "# if exists(f\"vector-dbs/{filename}\"):\n",
    "#     print(\"Embeddings already exist!\")\n",
    "#     vectorstore = get_vectorstore(embedder=embedder, local_vector_path=filename)\n",
    "# else:\n",
    "#     vectorstore = get_vectorstore(embedder=embedder, documents=docs)\n",
    "#     save_vector(vectorstore, filename)\n",
    "#     print(f\"Embeddings saved to vector-dbs/{filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_vectorstore(embedder, collection_name = \"test_collection\", docs = None):\n",
    "    \"\"\"\n",
    "    Create a vectorstore from documents\n",
    "    \"\"\"\n",
    "    filename = f\"chroma-vector-dbs/{collection_name}\"\n",
    "    is_local = False\n",
    "    if exists(filename):\n",
    "        print(\"Note: Collection seems to already exist! Not adding documents to the collection.\")\n",
    "        is_local = True\n",
    "    else:\n",
    "        if docs is None:\n",
    "            raise ValueError(\"Collection not found. Provide documents to create a new collection\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name, \n",
    "        embedding_function=embedder,\n",
    "        persist_directory=filename,\n",
    "        client_settings= Settings(anonymized_telemetry=False, is_persistent=True),\n",
    "    )\n",
    "    # if is_local is False:\n",
    "    #     vectorstore.add_documents(docs)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = create_chroma_vectorstore(embedder, collection_name=\"test_collection\", docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "for doc in docs:\n",
    "    # doc.metadata[\"doc_id\"] = doc_ids.pop(0)\n",
    "    doc.metadata = {\"doc_id\": doc_ids.pop(0)}\n",
    "    print(doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.similarity_search(\"What is the massed repetition?\", k=2)\n",
    "# vectorstore.similarity_search_with_relevance_scores(\"What is the spacing effect?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the file iceAge.json\n",
    "# import json\n",
    "# with open('iceAge.json', 'r') as myfile:\n",
    "#     data=myfile.read()\n",
    "# # parse file\n",
    "# obj = json.loads(data)\n",
    "# transcript = obj[\"output\"][\"text\"]\n",
    "\n",
    "# print(transcript)\n",
    "# # save to transcript.txt\n",
    "# with open('transcript.txt', 'w') as file:\n",
    "#     file.write(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific steps:\n",
    "1. Add document to appropriate folder in repo\n",
    "2. Set config.py with values for: \n",
    "    - document name\n",
    "    - chunk size\n",
    "    - overlap\n",
    "3. Assert that document is present and vectorstore doesn't already exist\n",
    "4. Create vectorstore using config values\n",
    "5. Save initial vectorstore\n",
    "6. Add Q&A or questions as metadata for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load vectorstore\n",
    "# # The vectorstore to use to index the child chunks\n",
    "# vectorstore = Chroma(collection_name=\"full_documents\", embedding_function=embedder,\n",
    "#                      client_settings= Settings( anonymized_telemetry=False, is_persistent=True, )\n",
    "# )\n",
    "# # The storage layer for the parent documents\n",
    "# store = InMemoryByteStore()\n",
    "# id_key = \"doc_id\"\n",
    "# # The retriever (empty to start)\n",
    "# retriever = MultiVectorRetriever(\n",
    "#     vectorstore=vectorstore,\n",
    "#     byte_store=store,\n",
    "#     id_key=id_key,\n",
    "# )\n",
    "# import uuid\n",
    "\n",
    "# doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The splitter to use to create smaller chunks\n",
    "# child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_docs = []\n",
    "# for i, doc in enumerate(docs):\n",
    "#     _id = doc_ids[i]\n",
    "#     _sub_docs = child_text_splitter.split_documents([doc])\n",
    "#     for _doc in _sub_docs:\n",
    "#         _doc.metadata[id_key] = _id\n",
    "#     sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.vectorstore.add_documents(sub_docs)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.vectorstore.similarity_search(\"spaced memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents(\"What is the role of spaced memory?\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_questions: list[list[str]] = []\n",
    "for i in range(len(docs)):\n",
    "    questions = []\n",
    "    questions.append(f\"What is example question {i}?\")\n",
    "    questions.append(f\"Is this example question {i}?\")\n",
    "    hypothetical_questions.append(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\", embedding_function=embedder,\n",
    "    client_settings= Settings( anonymized_telemetry=False, is_persistent=True, )\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "assert len(hypothetical_questions) == len(docs)\n",
    "question_docs = []\n",
    "for i, question_list in enumerate(hypothetical_questions):\n",
    "    question_docs.extend(\n",
    "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in docs:\n",
    "    if i is not None:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in question_docs:\n",
    "    if i is not None:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = []\n",
    "for docs in docs:\n",
    "    new_doc = Document(page_content=doc.page_content, metadata={id_key: doc_ids[i]})\n",
    "    new_docs.append(new_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(docs)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.similarity_search(\"What is massed repetition?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_docs(_dict):\n",
    "    # print(\"now attempting to print docs\")\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in _dict[\"context\"]])\n",
    "    # print(_dict[\"context\"])\n",
    "    _dict[\"context\"] = context\n",
    "    # return _dict[\"context\"]\n",
    "    print(context)\n",
    "    return _dict\n",
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "prompt.messages.insert(0, \n",
    "   SystemMessage(\n",
    "       content=\"You are an expert AI. Answer to the best of your ability, following the instructions.\"\n",
    "   )\n",
    "  )\n",
    "retrieved_docs = {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(print_docs)\n",
    "    | prompt\n",
    "    | ACTIVE_LLM\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is spaced practice and massed practice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
